# -*- coding: utf-8 -*-
"""1215654_Foram.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hpEodXgYU0mmyStqp1RXOQMrHmTcwcq5
"""

import numpy as np
from collections import defaultdict, Counter
import math
from sklearn.preprocessing import normalize
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from typing import List

class CustomTFIDF:
    def __init__(self, data: List[List[str]]):
        #data is a list of lists which consists of words. For example = [["it", "consist", "of", "words"]]
        self.data = data
        #Map to store words to indexes in the vocab.
        self.word_to_index = {}
        #Map to store inverse document frequency for each unique word in vocab.
        self.idfs_ = {}
        #Vocab stores all the unique words in the dataset.
        self.vocab = set()

    def _build_vocab(self):
        """Method to build vocabulary. Vocabulary is a list of unique words in the dataset and it is alphabetically sorted.
        Hint: Use the self.vocab to store unique words
        Note: Include only words whose length is >= 2
        """
        for data in self.data:
            for word in data:
                if len(word) >= 2 and word not in self.vocab:
                    self.vocab.add(word)
        #Implement your logic above this line
        #Do not modify the below line
        self.vocab = sorted(list(self.vocab))

    def create_index_map(self):
        """Method to map every word to its index in the vocabulary
        Hint: Iterate over the vocabulary and store them in word_to_index
        """
        #Implement the logic below this line
        for i, word in enumerate(self.vocab):
            self.word_to_index[word] = i

    def calculate_idfs(self):
        """Method to calculate inverse document frequency. Use the below formula to calculate inverse document frequency
        idf = 1 + math.log(1 + length of data) / (1 + total number of documents with term "t" in it)
        Reference: http://www.tfidf.com/
        Hint:
            Iterate over the vocab and check if word occurs in sentence. Count the occurence and store them in self.idfs_ dictionary.
        """

        #implement your logic below this line
        length_of_data = len(self.data)
        word_count = {}
        for word in self.vocab:
            for data in self.data:
                if word in data:
                    if word not in word_count.keys():
                        word_count[word] = 1
                    else:
                        word_count[word] += 1
            self.idfs_[word] = 1 + ( math.log( (1 + length_of_data)/(word_count[word] + 1) ) )

    def tfidf(self, input_sent: List[str]) -> np.ndarray:
        """Method which accepts input_sent of the form : ["is", "this", "a", "sent"].
        Task:
            Implement the tfidf approach for the above input.
            This function should return a vector of counts for the given inputs.
            The output shape of the vector should be 1 x len(self.vocab)
            The output should be a numpy array
            Example:
                length of vocabulary = 10
                input_sent = ["is", "this", "a", "sent"]
                The method should return a vector of shape 1 x 10.
            Formula:
                TF(t) = (Number of times word t appears in a document) / (Total number of words in the document).
                IDF will already be calculated and stored in self.idfs_ variable.
        """

        #Implement your logic below this line
        vector = np.zeros(len(self.vocab))
        for word in input_sent:
            if word in self.word_to_index:
                word_count_document = Counter(input_sent)
                tf = word_count_document[word] / len(input_sent)
                vector[self.word_to_index[word]] = tf * self.idfs_[word]
        return vector

    def create_vector(self):
        #Do not modify this function
        #hint: if you understand this function, you will understand what steps you will need to implement first.
        self._build_vocab()
        self.create_index_map()
        self.calculate_idfs()
        vector = []
        for sent in self.data:
            sent_vector = self.tfidf(sent)
            vector.append(sent_vector)
        vector = np.array(vector)
        vector = normalize(vector)
        return vector



#Do not modify code below this. If this modified you will be given straight away 0.
class testApproach:
    def __init__(self):
        self.original_corpus = ["this is a document", "this is a processed document", "is this a document", "This is not a document"]
        self.corpus = self.process(self.original_corpus)
        self.sklearn_bow = CountVectorizer()
        self.sklearn_transformer = TfidfTransformer()
        self.custom_tfidf = CustomTFIDF(self.corpus)

    def process(self, corpus):
        corpus = [x.lower() for x in corpus]
        return [x.split() for x in corpus]

    def testTFIDF(self):
        sklearn_output = self.sklearn_bow.fit_transform(self.original_corpus)
        sklearn_output = self.sklearn_transformer.fit_transform(sklearn_output).toarray()
        custom_output = self.custom_tfidf.create_vector()
        try:
            is_correct = np.allclose(sklearn_output, custom_output)
        except:
            is_correct = False
        if is_correct is True:
            print(f"ID : {ID} | Name : {NAME} | ALL test cases passed.")
            print("===="*20)
        else:
            print(f"ID : {ID} | Name : {NAME} | ALL test cases Failed.")
            print("===="*20)

if __name__ == "__main__":
    tester = testApproach()
    tester.testTFIDF()